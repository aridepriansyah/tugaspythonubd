{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRZz+YMqMqiAAId8mlNWzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrbarokah/Python/blob/master/1_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYmp622rh-tc",
        "colab_type": "text"
      },
      "source": [
        "##**1. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H07VVcmgPm3O",
        "colab_type": "text"
      },
      "source": [
        "* VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
        "Source : https://github.com/cjhutto/vaderSentiment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZC7sIOvhLkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Library\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj-iCWSNQ4KB",
        "colab_type": "text"
      },
      "source": [
        "* Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
        "* NLTK is a leading platform for building Python programs to work with human language data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akevYOvYhMqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCGAzlQqh5_m",
        "colab_type": "text"
      },
      "source": [
        "##**2. Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s4No_aAdnc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data from Github\n",
        "url = 'https://raw.githubusercontent.com/mrbarokah/Python/master/dataset/Text_GeneralMotor.csv'\n",
        "df = pd.read_csv(url, sep=',',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTiM5zNweBng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uz7p18EefJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Nama_File.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYWIffzhb4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data if Normal\n",
        "df = pd.read_csv(\"/content/Nama_File.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoNKFGqKRWvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Normal Data if UnicodeError Occured\n",
        "df = pd.read_csv(\"/content/Nama_File.csv\", encoding = \"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7pG-XNthyOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huk6nZDjh15j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce6eIbGKlvl_",
        "colab_type": "text"
      },
      "source": [
        "#**3. PreProcessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvMQYxnNBXAI",
        "colab_type": "text"
      },
      "source": [
        "###a. Remove Duplicate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p93BaJ8_i9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Duplicate Row from Table\n",
        "df = df.drop_duplicates()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxHScdi48YC4",
        "colab_type": "text"
      },
      "source": [
        "###b. RemoveURL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvj60dL8WIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove Duplicate from Selected Column\n",
        "df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMmoXQbwxK0",
        "colab_type": "text"
      },
      "source": [
        "###c. LowerCasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzi6mWuvulb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merubah keseluruhan kalimat di kolom yang dipilih menjadi huruf kecil\n",
        "df['text'] = df['text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8pT1YApwrth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GR5iPwH_wsU",
        "colab_type": "text"
      },
      "source": [
        "###d. RemoveUsername (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZRgD1h_0Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Menghilangkan kata yang diawali oleh simbol @ pada kolom tertentu\n",
        "df['text'] = df['text'].str.replace('@[^\\s]+','', case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7GNk15qxhLJ",
        "colab_type": "text"
      },
      "source": [
        "###e. Tokenize (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtudKVrzUdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-_m0d_xj8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing\n",
        "example_text = df.iloc[0]\n",
        "print(nltk.word_tokenize(example_text['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ianpkdFyzQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identify_tokens(row):\n",
        "    text = row['text']\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "\n",
        "df['text'] = df.apply(identify_tokens, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1D8eJsuzB-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0m1ExWzg87",
        "colab_type": "text"
      },
      "source": [
        "###f. Stemming (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyKjBwLhzjFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zz2ArPmzlht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_list = ['frightening', 'frightened', 'frightens']\n",
        "print ([stemming.stem(word) for word in my_list])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4KljGvi0gj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_list(row):\n",
        "    my_list = row['text']\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
        "    return (stemmed_list)\n",
        "\n",
        "df['stemmed_words'] = df.apply(stem_list, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JofG8_AP0ss2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHW4Og7VrrSO",
        "colab_type": "text"
      },
      "source": [
        "###g. Stopwords (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHzEbFA1SvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na6chvUW1YHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stops(row):\n",
        "    text = row['text']\n",
        "    meaningful_words = [w for w in text if not w in stops]\n",
        "    return (meaningful_words)\n",
        "\n",
        "df['text'] = df.apply(remove_stops, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wy5D-4vR9ZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Joining Text\n",
        "df['text'] = df['text'].str.join(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T925fp31h18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df['text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkOE2fChzFFy",
        "colab_type": "text"
      },
      "source": [
        "###h. Special Character (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxrqp4be7YkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "printable = set(string.printable)\n",
        "\n",
        "def remove_spec_chars(in_str):\n",
        "    return ''.join([c for c in in_str if c in printable])\n",
        "\n",
        "data['text'].apply(remove_spec_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKXeNt32lnoM",
        "colab_type": "text"
      },
      "source": [
        "#**4. Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9i3qzQsDPc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change Title to String\n",
        "df['text'] = df['text'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEa3xwALhD2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dISYmHJxhHzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in df.iterrows():\n",
        "  df['text']\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "df['polarity'] = se.values\n",
        "display(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QCKCoqHz7nn",
        "colab_type": "text"
      },
      "source": [
        "###a. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvN0c3J9zvPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie Chart\n",
        "import matplotlib.pyplot as plt\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_YHFkszUxI",
        "colab_type": "text"
      },
      "source": [
        "###b. Save to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2scLhNwqhS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('Output_File.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}